{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\modar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\modar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\modar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from preprocess import prepare\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path):\n",
    "    with open(path,\"r\",encoding=\"utf-8\") as f:\n",
    "        lines=f.readlines()\n",
    "    doc=[line for line in lines if line!=\"\\n\"]\n",
    "    doc=\" \".join(doc)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_inDic(text,filename,category):\n",
    "    dic={\"address\": str(f\"{filename}\"), \"html\": text, \"hash_code\": str(f\"{filename}\")}\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#passing the texts to the prepare method to modify them for the search\n",
    "def prepare_files(path,filename,category,df):\n",
    "    doc=read_file(path)\n",
    "    dic=save_inDic(doc,filename,category)\n",
    "    prepare(dic,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=[\"address\",\"html\",\"hash_code\",\"embedding_index\",\"mean_vector\"])\n",
    "if not os.path.isfile(\"back_up/modified_data_redundant.csv\"):\n",
    "    df.to_csv(\"back_up/modified_data_redundant.csv\",index=False)\n",
    "df=pd.read_csv(\"back_up/modified_data_redundant.csv\")\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: tech 401.txt        t    \r"
     ]
    }
   ],
   "source": [
    "#reading the texts from the BBC News dataset\n",
    "categories=[\"business\",\"entertainment\",\"politics\",\"sport\",\"tech\"]\n",
    "for category in categories:\n",
    "    folder_path= f\"bbc-fulltext/bbc/{category}\"\n",
    "    for filename in os.listdir(folder_path):\n",
    "        path = f\"bbc-fulltext/bbc/{category}/{filename}\"\n",
    "        prepare_files(path,filename,category,df)\n",
    "        print(f\"Processing file: {category} {filename}    \", end=\"\\r\")\n",
    "    df.to_csv(\"back_up/modified_data_redundant.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wiki = pd.DataFrame(columns=[\"address\",\"html\",\"hash_code\",\"embedding_index\",\"mean_vector\"])\n",
    "if not os.path.isfile(\"back_up/modified_data_wiki.csv\"):\n",
    "    df_wiki.to_csv(\"back_up/modified_data_wiki.csv\",index=False)\n",
    "df_wiki=pd.read_csv(\"back_up/modified_data_wiki.csv\")\n",
    "len(df_wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: 150.txt \r"
     ]
    }
   ],
   "source": [
    "#read the texts from the wikipedia dataset\n",
    "folder_path= f\"back_up/wikis\"\n",
    "for filename in sorted(os.listdir(folder_path),key=lambda x: int(x.split(\".\")[0])):\n",
    "    path = f\"back_up/wikis/{filename}\"\n",
    "    prepare_files(path,filename,\"category\",df_wiki)\n",
    "    print(f\"Processing file: {filename} \", end=\"\\r\")\n",
    "df_wiki.to_csv(\"back_up/modified_data_wiki.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import random\n",
    "import re\n",
    "from nltk.corpus import stopwords \n",
    "import nltk\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\modar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(sent):\n",
    "    cleaned_sentence = re.sub(r'\\b\\d+\\b', '', sent)\n",
    "    tokens = sent.split()\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    no_stop_words = [token for token in tokens\n",
    "                      if token not in stop_words\n",
    "                        and token.isalpha()]\n",
    "    cleaned_sentence = \" \".join(no_stop_words)\n",
    "    english_letters_pattern = re.compile(r'[a-zA-Z]+')\n",
    "    english_letters = english_letters_pattern.findall(cleaned_sentence)\n",
    "    cleaned_text = ' '.join(english_letters)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generates a list of sentences from the passed text\n",
    "def extract_sents(doc):\n",
    "    #a=[e.split(\".\") for e  in  doc]\n",
    "    a=[nltk.sent_tokenize(e) for e in doc]\n",
    "    flattened_list = [item for sublist in a for item in sublist if item!=\"\\n\"]\n",
    "    cleaned_doc= [clean_text(sentence) for sentence in flattened_list if sentence!=\"\"]\n",
    "    return [sent for sent in cleaned_doc if sent!=\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'us': 11,\n",
       " 'raising': 8,\n",
       " 'millions': 6,\n",
       " 'dollars': 3,\n",
       " 'african': 0,\n",
       " 'famine': 4,\n",
       " 'the': 10,\n",
       " 'also': 1,\n",
       " 'marks': 5,\n",
       " 'anniversary': 2,\n",
       " 'original': 7,\n",
       " 'recording': 9}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# v=TfidfVectorizer()\n",
    "# transformed=v.fit_transform(extract_sents([\"US and UK, raising millions of dollars for African famine relief. The re-release also marks the 20th anniversary of the original recording\"]))\n",
    "# vectors= transformed.toarray()\n",
    "# v.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['US raising millions dollars African famine',\n",
       " 'The also marks anniversary original recording']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extract_sents([\"US and UK, raising  millions of dollars for African famine relief. The re-release also marks the 20th anniversary of the original recording\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the text file from a given path and save it to  list of lines\n",
    "def read_for_query(path):\n",
    "    with open(path,\"r\", encoding=\"utf-8\") as f:\n",
    "        lines=f.readlines()\n",
    "    doc=[line for line in lines if line!=\"\\n\"]\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply TF-IDF on a given document and extract cndidates for the query\n",
    "def get_query_samples(doc):\n",
    "    v=TfidfVectorizer()\n",
    "    transformed=v.fit_transform(doc)\n",
    "    vectors= transformed.toarray()\n",
    "    mean=[]\n",
    "    keys=list(v.vocabulary_.keys())\n",
    "    for word in keys:\n",
    "        values=[vectors[i][v.vocabulary_.get(word)]\n",
    "         for i in range(len(vectors))]\n",
    "        mean.append((word,sum(values)/len(values)))\n",
    "\n",
    "    mean=sorted(mean,key= lambda x:x[1],reverse=True)\n",
    "    query_samples=[mean[r][0] for r in random.sample(range(101), 100)]\n",
    "    return query_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_to_csv(query,filename,category,df):\n",
    "#     document=str(f\"{category}{filename}\")\n",
    "#     df.loc[len(df)]=[document,query]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generates query for a given document for bbc News dataset\n",
    "def generate_queries_for_BBC(category,filename,df):\n",
    "    path = f\"bbc-fulltext/bbc/{category}/{filename}\"\n",
    "    \n",
    "\n",
    "    file=f\"{filename}\"\n",
    "    if file in list(df[\"document\"]):\n",
    "        return\n",
    "\n",
    "    doc=read_for_query(path)\n",
    "    sents=extract_sents(doc)\n",
    "    samples=[word for word in get_query_samples(sents)]\n",
    "    #print(f\"Processing file: {filename}    \", end=\"\\r\")\n",
    "    df.loc[len(df)]= [f\"{filename}\",\" \".join(samples)]\n",
    "    df.at[len(df)-1,\"document\"]=f\"{category}{filename}\"\n",
    "    df.at[len(df)-1,\"query\"]=\" \".join(samples)\n",
    "    # q=\" \".join(samples)\n",
    "    # print(f\"query: {q}    \", end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generates query for a given document for wikipedia dataset\n",
    "def generate_queries_for_wiki(filename,df):\n",
    "    path = f\"back_up/wikis/{filename}\"\n",
    "    \n",
    "\n",
    "    file=f\"{filename}\"\n",
    "    if file in list(df[\"document\"]):\n",
    "        return\n",
    "\n",
    "    doc=read_for_query(path)\n",
    "    sents=extract_sents(doc)\n",
    "    samples=[word for word in get_query_samples(sents)]\n",
    "    #print(f\"Processing file: {filename}    \", end=\"\\r\")\n",
    "    df.loc[len(df)]= [f\"{filename}\",\" \".join(samples)]\n",
    "    # df.at[len(df)-1,\"document\"]=f\"{category}{filename}\"\n",
    "    # df.at[len(df)-1,\"query\"]=\" \".join(samples)\n",
    "    #q=\" \".join(samples)\n",
    "    #print(f\"query: {q}    \", end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(columns=[\"document\",\"query\"])\n",
    "if not os.path.isfile(\"back_up/queries_wiki.csv\"):\n",
    "    df.to_csv(\"back_up/queries_wiki.csv\",index=False)\n",
    "df=pd.read_csv(\"back_up/queries_wiki.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: 140.txt    \r"
     ]
    }
   ],
   "source": [
    "# generating queries for all documents in the given directory\n",
    "#category=\"tech\"\n",
    "folder_path= \"back_up/wikis\"\n",
    "for filename in sorted(os.listdir(folder_path),key=lambda x: int(x.split(\".\")[0])):\n",
    "    path = f\"back_up/wikis/{filename}\"\n",
    "    generate_queries_for_wiki(filename,df)\n",
    "    print(f\"Processing file: {filename}    \", end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"back_up/queries_wiki.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the queries to suite both models fastText and Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim.downloader\n",
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "model=fasttext.load_model(\"cc.en.300.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>query</th>\n",
       "      <th>mean_vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.txt</td>\n",
       "      <td>government total august south robert national ...</td>\n",
       "      <td>[-0.02220891, -0.008327238, -0.015622603, 0.04...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.txt</td>\n",
       "      <td>white health retrieved world american election...</td>\n",
       "      <td>[-0.02980545, 0.011205166, -0.010511813, 0.053...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.txt</td>\n",
       "      <td>prince since on queen prime also bbc took in r...</td>\n",
       "      <td>[-0.01771623, 0.006959884, -0.018862734, 0.054...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.txt</td>\n",
       "      <td>culture census business january many congress ...</td>\n",
       "      <td>[-0.01835768, -0.0015837983, -0.01926455, 0.04...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.txt</td>\n",
       "      <td>messi in serie october mark united his decembe...</td>\n",
       "      <td>[-0.0088726785, -0.0036963962, -0.04611436, 0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  document                                              query  \\\n",
       "0    0.txt  government total august south robert national ...   \n",
       "1    1.txt  white health retrieved world american election...   \n",
       "2    2.txt  prince since on queen prime also bbc took in r...   \n",
       "3    3.txt  culture census business january many congress ...   \n",
       "4    4.txt  messi in serie october mark united his decembe...   \n",
       "\n",
       "                                            mean_vec  \n",
       "0  [-0.02220891, -0.008327238, -0.015622603, 0.04...  \n",
       "1  [-0.02980545, 0.011205166, -0.010511813, 0.053...  \n",
       "2  [-0.01771623, 0.006959884, -0.018862734, 0.054...  \n",
       "3  [-0.01835768, -0.0015837983, -0.01926455, 0.04...  \n",
       "4  [-0.0088726785, -0.0036963962, -0.04611436, 0....  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_queries=pd.read_csv(\"back_up/queries_wiki.csv\")\n",
    "fasttext_queries=pd.read_csv(\"back_up/queries_wiki_fasttext.csv\")\n",
    "gensim_queries=pd.read_csv(\"back_up/queries_wiki_gensim.csv\")\n",
    "fasttext_queries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_queries[\"mean_vec\"]=[[] for i in range(0,len(df_queries))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_vector_gensim(input):\n",
    "    keys=[x for x in input.split() if training_set.__contains__(x)]\n",
    "    meanVec=np.mean(np.array(list(map(lambda x :\n",
    "                                       training_set.get_vector(x) ,\n",
    "                                         keys))),axis=0)\n",
    "    return meanVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_vecs_fasttext(df):\n",
    "    df[\"mean_vec\"] = df[\"query\"].apply(lambda x: list(np.mean([model.get_word_vector(w) for w in x.split()], axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_vecs_fasttext(fasttext_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbc_queries_fasttext=pd.read_csv(\"back_up/labeld_queries_fasttext.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_vecs_fasttext(bbc_queries_fasttext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbc_queries_fasttext.to_csv(\"back_up/labeld_queries_fasttext.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fasttext_queries.to_csv(\"back_up/queries_wiki_fasttext.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>query</th>\n",
       "      <th>mean_vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.txt</td>\n",
       "      <td>government total august south robert national ...</td>\n",
       "      <td>[0.0403925, -0.40450022, -0.15040405, -0.09556...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.txt</td>\n",
       "      <td>white health retrieved world american election...</td>\n",
       "      <td>[0.037646394, -0.39482304, -0.14960004, -0.095...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.txt</td>\n",
       "      <td>prince since on queen prime also bbc took in r...</td>\n",
       "      <td>[0.04185474, -0.4012402, -0.15507908, -0.10232...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.txt</td>\n",
       "      <td>culture census business january many congress ...</td>\n",
       "      <td>[0.038593266, -0.40238896, -0.14885493, -0.098...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.txt</td>\n",
       "      <td>messi in serie october mark united his decembe...</td>\n",
       "      <td>[0.045165416, -0.40830454, -0.15094325, -0.100...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  document                                              query  \\\n",
       "0    0.txt  government total august south robert national ...   \n",
       "1    1.txt  white health retrieved world american election...   \n",
       "2    2.txt  prince since on queen prime also bbc took in r...   \n",
       "3    3.txt  culture census business january many congress ...   \n",
       "4    4.txt  messi in serie october mark united his decembe...   \n",
       "\n",
       "                                            mean_vec  \n",
       "0  [0.0403925, -0.40450022, -0.15040405, -0.09556...  \n",
       "1  [0.037646394, -0.39482304, -0.14960004, -0.095...  \n",
       "2  [0.04185474, -0.4012402, -0.15507908, -0.10232...  \n",
       "3  [0.038593266, -0.40238896, -0.14885493, -0.098...  \n",
       "4  [0.045165416, -0.40830454, -0.15094325, -0.100...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_queries.to_csv(\"back_up/queries_wiki_fasttext.csv\",index=False)\n",
    "fasttext_queries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after computing the mean vectors for the fastext model now we compute tthem for word2vec\n",
    "for i,query in enumerate(df_queries[\"query\"]):\n",
    "    df_queries.at[i,\"mean_vec\"]=list(mean_vector_gensim(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.txt</td>\n",
       "      <td>government total august south robert national ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.txt</td>\n",
       "      <td>white health retrieved world american election...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.txt</td>\n",
       "      <td>prince since on queen prime also bbc took in r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.txt</td>\n",
       "      <td>culture census business january many congress ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.txt</td>\n",
       "      <td>messi in serie october mark united his decembe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  document                                              query\n",
       "0    0.txt  government total august south robert national ...\n",
       "1    1.txt  white health retrieved world american election...\n",
       "2    2.txt  prince since on queen prime also bbc took in r...\n",
       "3    3.txt  culture census business january many congress ...\n",
       "4    4.txt  messi in serie october mark united his decembe..."
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_queries.to_csv(\"back_up/queries_wiki_gensim.csv\",index=False)\n",
    "gensim_queries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>query</th>\n",
       "      <th>mean_vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.txt</td>\n",
       "      <td>world the united april states archived origina...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.txt</td>\n",
       "      <td>april donald march michael november august oct...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.txt</td>\n",
       "      <td>philip pimlott the september queen in archived...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.txt</td>\n",
       "      <td>university june the february september may dec...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.txt</td>\n",
       "      <td>february may ronaldo madrid march october arch...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  document                                              query mean_vec\n",
       "0    0.txt  world the united april states archived origina...       []\n",
       "1    1.txt  april donald march michael november august oct...       []\n",
       "2    2.txt  philip pimlott the september queen in archived...       []\n",
       "3    3.txt  university june the february september may dec...       []\n",
       "4    4.txt  february may ronaldo madrid march october arch...       []"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
